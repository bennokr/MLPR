
import gzip
import cPickle
from copy import copy
import numpy as np
import matplotlib.pyplot as plt
from math import exp, log

from time import time

def load_mnist():
    f = gzip.open('mnist.pkl.gz', 'rb')
    data = cPickle.load(f)
    f.close()
    return data

def plot_digits(data, numcols, shape=(28,28)):
    numdigits = data.shape[0]
    numrows = int(numdigits/numcols)
    for i in range(numdigits):
        plt.subplot(numrows, numcols, i)
        plt.axis('off')
        plt.imshow(data[i].reshape(shape), interpolation='nearest', cmap='Greys')
    plt.show()

def plot_digits2(data, numcols, shape=(28,28)):
    data = data.T
    numdigits = len(data)
    numrows = int(numdigits/numcols)
    for i in range(numdigits):
        plt.subplot(numrows, numcols, i)
        plt.axis('off')
        plt.imshow(data[i].reshape(shape), interpolation='nearest', cmap='Greys')
    plt.show()
    
"assignemnt 1.112 where the fucntions of 1.111 are filled in. %TODO: if something changes in 1.111 we have to change this formula!" 
def logreg_gradient(x, t, w, b):
    matrixW = np.zeros(w.shape).T #transposed to fill the matrix easier (per row instead of collum).
    vectorB = np.zeros(b.shape).T
    for collum in xrange(len(matrixW)):
        Z = sum([exp(w.T[k].dot(x) + b[k]) for k in xrange(len(matrixW))])
        q = exp(w.T[collum].dot(x) + b.T[collum])
        if collum ==t:
            gradW = x - ((q*x)/Z)
            gradB = 1-(q/Z)
        else:
            gradW = -((q*x)/Z)
            gradB = -(q/Z)
        vectorB[collum] = gradB
        matrixW[collum] = gradW
    return matrixW.T,vectorB.T #transpose them back to give the right formula back.

"Assignment 1.1.3, maar mijn aannames moeten nog worden gecheckt."
def sgd_iter(x_train, t_train, w, b):
    learn = 0.0001
    all_indices = np.arange(len(x_train),dtype=int)
    np.random.shuffle(all_indices)
    for i in all_indices:
        gradW, gradB = logreg_gradient(x_train[i], t_train[i], w, b)
        w = w + learn*gradW
        b = b + learn*gradB
    return w,b

def logPData(x, t, w, b):
    Z = sum([exp(w.T[k].dot(x) + b[k]) for k in xrange(len(w.T))]) #W.T to count the colloms 
    q = exp(w.T[t].dot(x) + b.T[t])
    logP = log(q)-log(Z)
    return logP


def train(x_train, t_train, x_valid, t_valid, w, b):
    training = 4
    logP=np.zeros(training)
    logPvalid=np.zeros(training)
    last_iteration_values = []

    for i in xrange(training):
        w,b = sgd_iter(x_train, t_train, w, b)

        for j in xrange(len(x_train)):
            logP[i] = logP[i]+logPData(x_train[j], t_train[j], w,b)
        for j in xrange(len(x_valid)):
            v = logPData(x_valid[j], t_valid[j], w,b)
            logPvalid[i] = logPvalid[i] + v
            if i == training-1:
                last_iteration_values.append((x_valid[j], t_valid[j], v))

    logP = logP / len(x_train)
    logPvalid = logPvalid / len(x_valid)

    last_iteration_values.sort(key=lambda tup : tup[2])

    plot_digits(np.array(map(lambda x : x[0], last_iteration_values[0:8])), numcols = 4)
    plot_digits(np.array(map(lambda x : x[0], last_iteration_values[len(last_iteration_values)-8:])), numcols = 4)


    plt.plot(range(1,training+1),logP, range(1,training+1),logPvalid)
    plt.plot(range(1,training+1),logP,'o', range(1,training+1),logPvalid, 'bs')
    plt.show()

    #vraag 1.2.1
    plot_digits2(w, numcols=5)
    plt.show()

if __name__ == "__main__":
    start = time()
    ## Plot the digits of the given dataset
    (x_train, t_train), (x_valid, t_valid), (x_test, t_test) = load_mnist()

    #plot_digits(x_train[0:8], numcols=4)
    ## some data to test the function
    w = np.zeros((784, 10))
    b = np.zeros(10).T

    small = False
    if small != False:
        train(x_train[0:small], t_train[0:small], x_valid[0:small], t_valid[0:small], w, b)
    else:
        train(x_train, t_train, x_valid, t_valid, w, b)

    stop = time()

    print "Time: ", int(stop-start+0.5)


