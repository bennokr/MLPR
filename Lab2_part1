# -*- coding: utf-8 -*-
"""
Created on Tue Oct 01 08:47:47 2013

@author: Aiko
"""

# -*- coding: utf-8 -*-
"""
Spyder Editor

This temporary script file is located here:
c:\WINNT\profiles\10450793\.spyder2\.temp.py
"""
import gzip
import cPickle
from copy import copy
import numpy as np
import matplotlib.pyplot as plt
from math import exp, log

def load_mnist():
	f = gzip.open('mnist.pkl.gz', 'rb')
	data = cPickle.load(f)
	f.close()
	return data

def plot_digits(data, numcols, shape=(28,28)):
    numdigits = data.shape[0]
    numrows = int(numdigits/numcols)
    for i in range(numdigits):
        plt.subplot(numrows, numcols, i)
        plt.axis('off')
        plt.imshow(data[i].reshape(shape), interpolation='nearest', cmap='Greys')
    plt.show()

def plot_digits2(data, numcols, shape=(28,28)):
    data = data.T
    numdigits = len(data)
    numrows = int(numdigits/numcols)
    for i in range(numdigits):
        plt.subplot(numrows, numcols, i)
        plt.axis('off')
        plt.imshow(data[i].reshape(shape), interpolation='nearest', cmap='Greys')
    plt.show()
    
"assignemnt 1.112 where the fucntions of 1.111 are filled in. %TODO: if something changes in 1.111 we have to change this formula!" 
def logreg_gradient(x, t, w, b):
    matrixW = np.zeros(w.shape).T #transposed to fill the matrix easier (per row instead of collum).
    vectorB = np.zeros(b.shape).T
    for collum in xrange(len(matrixW)):
        Z = sum([exp(w.T[k].dot(x) + b[k]) for k in xrange(len(matrixW))])
        q = exp(w.T[collum].dot(x) + b.T[collum])
        if collum ==t:
            gradW = x - ((q*x)/Z)
            gradB = 1-(q/Z)
        else:
            gradW = -((q*x)/Z)
            gradB = -(q/Z)
        vectorB[collum] = gradB
        matrixW[collum] = gradW
    return matrixW.T,vectorB.T #transpose them back to give the right formula back.

"Assignment 1.1.3, maar mijn aannames moeten nog worden gecheckt."
def sgd_iter(x_train, t_train, w, b):
    learn = 0.0001
    all_indices = np.arange(len(x_train),dtype=int)
    np.random.shuffle(all_indices)
    for i in all_indices:
        gradW, gradB = logreg_gradient(x_train[i], t_train[i], w, b)
        w = w + learn*gradW
        b = b + learn*gradB
    return w,b

def logPData(x, t, w, b):
    Z = sum([exp(w.T[k].dot(x) + b[k]) for k in xrange(len(w.T))]) #W.T to count the colloms 
    q = exp(w.T[t].dot(x) + b.T[t])
    logP = log(q)-log(Z)
    return logP

def train(x_train, t_train,x_valid, t_valid, w, b):
    training =1
    logP=np.zeros(training)
    logPvalid=np.zeros(training)
    for i in xrange(training):
        w,b = sgd_iter(x_train, t_train, w, b)
        for data in xrange(len(x_train)):
            logP[i] = logP[i]+logPData(x_train[i],t_train_w,b)
        for data in xrange(len(x_valid)):
            logPvalid[i] = logPvalid[i]+logPData(x_valid[i],t_valid,w,b)
    plt.plot(range(1,training+1),logP,'o',range(1,training+1),logPvalid,'bs')
    #vraag 1.2.1
    plot_digits2(w, numcols=5)
    plt.show()
                
if __name__ == "__main__":
    ## Plot the digits of the given dataset
    (x_train, t_train), (x_valid, t_valid), (x_test, t_test) = load_mnist()
    #plot_digits(x_train[0:8], numcols=4)
    ## some data to test the function
    print log(exp(10))
    w = np.zeros((784, 10))
    b = np.array([0,0,0,0,0,0,0,0,0,0]).T
    sgd_iter(x_train, t_train, w, b)
    train(x_train[0:1000], t_train[0:1000], x_valid, t_valid, w, b)
