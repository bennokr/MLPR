# -*- coding: utf-8 -*-
"""
Created on Tue Oct 01 08:47:47 2013

@author: Aiko
"""

# -*- coding: utf-8 -*-
"""
Spyder Editor

This temporary script file is located here:
c:\WINNT\profiles\10450793\.spyder2\.temp.py
"""
import gzip
import cPickle
from copy import copy
import numpy as np
import matplotlib.pyplot as plt
from math import exp, log

def load_mnist():
	f = gzip.open('mnist.pkl.gz', 'rb')
	data = cPickle.load(f)
	f.close()
	return data

def plot_digits(data, numcols, shape=(28,28)):
    numdigits = data.shape[0]
    numrows = int(numdigits/numcols)
    for i in range(numdigits):
        plt.subplot(numrows, numcols, i)
        plt.axis('off')
        plt.imshow(data[i].reshape(shape), interpolation='nearest', cmap='Greys')
    plt.show()

def plot_digits2(data, numcols, shape=(28,28)):
    data = data.T
    numdigits = len(data)
    numrows = int(numdigits/numcols)
    for i in range(numdigits):
        plt.subplot(numrows, numcols, i)
        plt.axis('off')
        plt.imshow(data[i].reshape(shape), interpolation='nearest', cmap='Greys')
    plt.show()
    
"assignemnt 1.112 where the fucntions of 1.111 are filled in. %TODO: if something changes in 1.111 we have to change this formula!" 
def logreg_gradient(x, t, w, b):
    matrixW = np.zeros(w.shape).T #transposed to fill the matrix easier (per row instead of collum).
    vectorB = np.zeros(b.shape).T
    for collum in xrange(len(matrixW)):
        Z = sum([exp(w.T[k].dot(x) + b[k]) for k in xrange(len(matrixW))])
        q = exp(w.T[collum].dot(x) + b.T[collum])
        if collum ==t:
            gradW = x - ((q*x)/Z)
            gradB = 1-(q/Z)
        else:
            gradW = -((q*x)/Z)
            gradB = -(q/Z)
        vectorB[collum] = gradB
        matrixW[collum] = gradW
    return matrixW.T,vectorB.T #transpose them back to give the right formula back.

"Assignment 1.1.3, maar mijn aannames moeten nog worden gecheckt."
def sgd_iter(x_train, t_train, w, b):
    learn = 0.0001
    all_indices = np.arange(len(x_train),dtype=int)
    np.random.shuffle(all_indices)
    print all_indices
    for i in all_indices: #%TODO He has to go trough the trainingset in a randomized order.
        gradW, gradB = logreg_gradient(x_train[i], t_train[i], w, b)
        newW = w + learn*gradW
        newB = b + learn*gradB
        w = newW
        b= newB
    return w,b
    

def train2(x_train, t_train,x_valid, t_valid, w, b):
    training =1
    lnP = np.zeros(training)
    for i in xrange(training):
        w,b = sgd_iter(x_train, t_train, w, b)
        for a in xrange(10):
            for sets in xrange(len(x_train)):
                if t_train[sets] == a:
                    Z = sum([exp(w.T[k].dot(x_train[sets]) + b[k]) for k in xrange(len(w.T))])
                    q = exp(w.T[a].dot(x_train[sets]) + b.T[a])
                    #%Op dit moment sommeer ik over alle T's en alle x's.
                    lnP[i] = lnP[i] + (log(q) - log(Z)) #%TODO checken of dit de natuurlijke logaritme is!
    plt.plot(np.arange(1, training+1, 1),lnP, 'o')
    #vraag 1.2.1
    plot_digits2(w, numcols=5)
    plt.show()
                
if __name__ == "__main__":
    ## Plot the digits of the given dataset
    (x_train, t_train), (x_valid, t_valid), (x_test, t_test) = load_mnist()
    #plot_digits(x_train[0:8], numcols=4)
    ## some data to test the function
    w = np.zeros((784, 10))
    b = np.array([0,0,0,0,0,0,0,0,0,0]).T
    sgd_iter(x_train, t_train, w, b)
    #train2(x_train, t_train, x_valid, t_valid, w, b)
